{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpeng/Applications/anaconda3/envs/dev37/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation\n",
      "Evaluation model fit in 15.787 seconds\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.78      0.81       207\n",
      "        pos       0.78      0.83      0.80       193\n",
      "\n",
      "avg / total       0.81      0.81      0.81       400\n",
      "\n",
      "Building complete model and saving ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mpeng/Applications/anaconda3/envs/dev37/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete model fit in 105.117 seconds\n",
      "Model written out to model.pickle\n"
     ]
    }
   ],
   "source": [
    "# An end-to-end demonstration of a Scikit-Learn SVM or XGBoost classifier\n",
    "# trained on the positive and negative movie reviews corpus in NLTK.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier, callback\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report as clsr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split as tts\n",
    "\n",
    "if False:\n",
    "    import nltk\n",
    "    nltk.download(\"stopwords\")\n",
    "    nltk.download(\"movie_reviews\")\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def timeit(func):\n",
    "    \"\"\"\n",
    "    Simple timing decorator\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        delta = time.time() - start\n",
    "        return result, delta\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transforms input data by using NLTK tokenization, lemmatization, and\n",
    "    other normalization and filtering techniques.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None, lower=True, strip=True):\n",
    "        \"\"\"\n",
    "        Instantiates the preprocessor, which make load corpora, models, or do\n",
    "        other time-intenstive NLTK data loading.\n",
    "        \"\"\"\n",
    "        self.lower = lower\n",
    "        self.strip = strip\n",
    "        self.stopwords = set(stopwords) if stopwords else set(\n",
    "            sw.words('english'))\n",
    "        self.punct = set(punct) if punct else set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit simply returns self, no other information is needed.\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\n",
    "        No inverse transformation\n",
    "        \"\"\"\n",
    "        return X\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Actually runs the preprocessing on each document.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        \"\"\"\n",
    "        Returns a normalized, lemmatized list of tokens from a document by\n",
    "        applying segmentation (breaking into sentences), then word/punctuation\n",
    "        tokenization, and finally part of speech tagging. It uses the part of\n",
    "        speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "        version of all the words, removing stopwords and punctuation.\n",
    "        \"\"\"\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "                # If punctuation or stopword, ignore token and continue\n",
    "                if token in self.stopwords or all(char in self.punct for char in token):\n",
    "                    continue\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        \"\"\"\n",
    "        Converts the Penn Treebank tag to a WordNet POS tag, then uses that\n",
    "        tag to perform much more accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "@timeit\n",
    "def build_and_evaluate(X, y, classifier=XGBClassifier, outpath=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Builds a classifer for the given list of documents and targets in two\n",
    "    stages: the first does a train/test split and prints a classifier report,\n",
    "    the second rebuilds the model on the entire corpus and returns it for\n",
    "    operationalization.\n",
    "    X: a list or iterable of raw strings, each representing a document.\n",
    "    y: a list or iterable of labels, which will be label encoded.\n",
    "    Can specify the classifier to build with: if a class is specified then\n",
    "    this will build the model with the Scikit-Learn defaults, if an instance\n",
    "    is given, then it will be used directly in the build pipeline.\n",
    "    If outpath is given, this function will write the model as a pickle.\n",
    "    If verbose, this function will print out information to the command line.\n",
    "    \"\"\"\n",
    "    def make_pipeline_before_model(X):\n",
    "        \"\"\"\n",
    "        Set a universal pipeline for model to train on\n",
    "        \"\"\"\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "        ])\n",
    "        model.fit(X)\n",
    "        return model\n",
    "\n",
    "    # Build Classifer\n",
    "    param_dist = {'objective': 'binary:logistic', 'n_estimators': 1000, 'subsample': 0.7,\n",
    "                  \"n_jobs\": -1, \"max_depth\": 2, \"reg_alpha\": 0.01,\n",
    "                  \"reg_lambda\": 1, \"random_state\": 123}\n",
    "\n",
    "    @timeit\n",
    "    def build_eval(classifier, X_train, y_train, X_test, y_test, param_dist=param_dist):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            classifier = classifier(**param_dist)\n",
    "        classifier.fit(X_train, y_train,\n",
    "                       eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                       eval_metric='logloss',\n",
    "                       verbose=False, early_stopping_rounds=10)\n",
    "        return classifier\n",
    "\n",
    "    # first make pipeline convert X, the list of strings/texts\n",
    "    # to the data matrix (conversion is done by *.transform operation)\n",
    "    feature_pipeline = make_pipeline_before_model(X)\n",
    "    # Label encode the targets\n",
    "    labels = LabelEncoder()\n",
    "    y = labels.fit_transform(y)\n",
    "\n",
    "    # Begin evaluation\n",
    "    X_model = feature_pipeline.transform(X)\n",
    "    if verbose:\n",
    "        print(\"Building for evaluation\")\n",
    "    X_train, X_test, y_train, y_test = tts(\n",
    "        X_model, y, test_size=0.2, random_state=123)\n",
    "    model, secs = build_eval(classifier, X_train, y_train, X_test, y_test)\n",
    "    if verbose:\n",
    "        print(\"Evaluation model fit in {:0.3f} seconds\".format(secs))\n",
    "    if verbose:\n",
    "        print(\"Classification Report:\\n\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(clsr(y_test, y_pred, target_names=labels.classes_))\n",
    "\n",
    "    @timeit\n",
    "    def build(classifier, X, y, m_=model, param_dist=param_dist):\n",
    "        \"\"\"\n",
    "        Inner build function that builds a single model.\n",
    "        \"\"\"\n",
    "        if isinstance(classifier, type):\n",
    "            param_dist['n_estimators'] = m_.best_iteration\n",
    "            classifier = classifier(**param_dist)\n",
    "        model = Pipeline([\n",
    "            ('preprocessor', NLTKPreprocessor()),\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                tokenizer=identity, preprocessor=None, lowercase=False)),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        model.fit(X, y, classifier__eval_metric='logloss')\n",
    "        return model\n",
    "    if verbose:\n",
    "        print(\"Building complete model and saving ...\")\n",
    "    model, secs = build(classifier, X, y)\n",
    "    model.labels_ = labels\n",
    "    if verbose:\n",
    "        print(\"Complete model fit in {:0.3f} seconds\".format(secs))\n",
    "    if outpath:\n",
    "        with open(outpath, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(\"Model written out to {}\".format(outpath))\n",
    "    return model\n",
    "\n",
    "\n",
    "def show_most_informative_features(model, text=None, n=20):\n",
    "    \"\"\"\n",
    "    Accepts a Pipeline with a classifer and a TfidfVectorizer and computes\n",
    "    the n most informative features of the model. If text is given, then will\n",
    "    compute the most informative features for classifying that text.\n",
    "    Note that this function will only work on linear models with coefs_\n",
    "    \"\"\"\n",
    "    # Extract the vectorizer and the classifier from the pipeline\n",
    "    vectorizer = model.named_steps['vectorizer']\n",
    "    classifier = model.named_steps['classifier']\n",
    "\n",
    "    # Check to make sure that we can perform this computation\n",
    "    if not hasattr(classifier, 'coef_'):\n",
    "        raise TypeError(\n",
    "            \"Cannot compute most informative features on {} model.\".format(\n",
    "                classifier.__class__.__name__\n",
    "            )\n",
    "        )\n",
    "    if text is not None:\n",
    "        # Compute the coefficients for the text\n",
    "        tvec = model.transform([text]).toarray()\n",
    "    else:\n",
    "        # Otherwise simply use the coefficients\n",
    "        tvec = classifier.coef_\n",
    "    # Zip the feature names with the coefs and sort\n",
    "    coefs = sorted(\n",
    "        zip(tvec[0], vectorizer.get_feature_names()),\n",
    "        key=itemgetter(0), reverse=True\n",
    "    )\n",
    "    topn = zip(coefs[:n], coefs[:-(n+1):-1])\n",
    "    # Create the output string to return\n",
    "    output = []\n",
    "    # If text, add the predicted value to the output.\n",
    "    if text is not None:\n",
    "        output.append(\"\\\"{}\\\"\".format(text))\n",
    "        output.append(\"Classified as: {}\".format(model.predict([text])))\n",
    "        output.append(\"\")\n",
    "    # Create two columns with most negative and most positive features.\n",
    "    for (cp, fnp), (cn, fnn) in topn:\n",
    "        output.append(\n",
    "            \"{:0.4f}{: >15}    {:0.4f}{: >15}\".format(cp, fnp, cn, fnn)\n",
    "        )\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "##################################################\n",
    "# TRAIN Review Sentiment Classifier\n",
    "##################################################\n",
    "model_path = \"model.pickle\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    # Time to build the model\n",
    "    from nltk.corpus import movie_reviews as reviews\n",
    "\n",
    "    X = [reviews.raw(fileid) for fileid in reviews.fileids()]\n",
    "    y = [reviews.categories(fileid)[0] for fileid in reviews.fileids()]\n",
    "\n",
    "    model = build_and_evaluate(X, y, outpath=model_path)\n",
    "\n",
    "else:\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "try:\n",
    "    # this only works for linear model\n",
    "    print(show_most_informative_features(model))\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn\\'t snag this one correctly . \\nthey seem to have taken this pretty neat concept , but executed it terribly . \\nso what are the problems with the movie ? \\nwell , its main problem is that it\\'s simply too jumbled . \\nit starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what\\'s going on . \\nthere are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \\nnow i personally don\\'t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film\\'s biggest problem . \\nit\\'s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \\nand do they make things entertaining , thrilling or even engaging , in the meantime ? \\nnot really . \\nthe sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn\\'t the make the film all that more entertaining . \\ni guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \\ni mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \\nokay , we get it . . . there \\nare people chasing her and we don\\'t know who they are . \\ndo we really need to see it over and over again ? \\nhow about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \\napparently , the studio took this film away from its director and chopped it up themselves , and it shows . \\nthere might\\'ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \\nthe actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \\nbut my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character\\'s unraveling . \\noverall , the film doesn\\'t stick because it doesn\\'t entertain , it\\'s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \\noh , and by the way , this is not a horror or teen slasher flick . . . it\\'s \\njust packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \\nit also wrapped production two years ago and has been sitting on the shelves ever since . \\nwhatever . . . skip \\nit ! \\nwhere\\'s joblo coming from ? \\na nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \\n',\n",
       " 'the happy bastard\\'s quick movie review \\ndamn that y2k bug . \\nit\\'s got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \\nlittle do they know the power within . . . \\ngoing for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \\nwe don\\'t know why the crew was really out in the middle of nowhere , we don\\'t know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don\\'t know why donald sutherland is stumbling around drunkenly throughout . \\nhere , it\\'s just \" hey , let\\'s chase these people around with some robots \" . \\nthe acting is below average , even from the likes of curtis . \\nyou\\'re more likely to get a kick out of her work in halloween h20 . \\nsutherland is wasted and baldwin , well , he\\'s acting like a baldwin , of course . \\nthe real star here are stan winston\\'s robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone\\'s brain . \\nso , if robots and body parts really turn you on , here\\'s your movie . \\notherwise , it\\'s pretty much a sunken ship of a movie . \\n']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['neg', 'neg']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X[:2])\n",
    "print(\"*\"*80)\n",
    "display(y[:2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
