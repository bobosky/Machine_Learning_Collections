<!DOCTYPE html>
<!-- saved from url=(0077)https://medium.com/media/c4370da553f0222c88aabe7942808a61?postId=6241635ae130 -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>tokenizer.py – Medium</title><meta name="description" content="GitHub is where people build software. More than 27 million people use GitHub to discover, fork, and contribute to over 80 million projects."><meta name="twitter:widgets:csp" content="on"><meta name="robots" content="noindex"><!--<base target="_blank">--><base href="." target="_blank"><style>body {text-rendering: optimizeLegibility; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; font-family: "ff-tisa-web-pro", Georgia, Cambria, "Times New Roman", Times, serif; font-weight: 400; color: #333332; font-size: 18px; line-height: 1.4; margin: 0; background-color: white; overflow: hidden;}iframe {max-width: 100%;}</style></head><body><style>.gist .gist-file { margin-bottom: 0 !important; }.gist { text-rendering: auto; }</style><script src="./c943e38e4d233d07cf38d1886b487ac2.js" charset="utf-8"></script><link rel="stylesheet" href="./gist-embed-2c768148c4d27cab10fe818942078e18.css"><div id="gist88957433" class="gist">
    <div class="gist-file">
      <div class="gist-data">
        <div class="js-gist-file-update-container js-task-list-container file-box">
  <div id="file-tokenizer-py" class="file">
    

  <div itemprop="text" class="blob-wrapper data type-python ">
      <table class="highlight tab-size js-file-line-container" data-tab-size="8">
      <tbody><tr>
        <td id="file-tokenizer-py-L1" class="blob-num js-line-number" data-line-number="1"></td>
        <td id="file-tokenizer-py-LC1" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> all imports</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L2" class="blob-num js-line-number" data-line-number="2"></td>
        <td id="file-tokenizer-py-LC2" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> collections <span class="pl-k">import</span> Counter</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L3" class="blob-num js-line-number" data-line-number="3"></td>
        <td id="file-tokenizer-py-LC3" class="blob-code blob-code-inner js-file-line"><span class="pl-k">import</span> spacy</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L4" class="blob-num js-line-number" data-line-number="4"></td>
        <td id="file-tokenizer-py-LC4" class="blob-code blob-code-inner js-file-line"><span class="pl-k">from</span> tqdm <span class="pl-k">import</span> tqdm, tqdm_notebook, tnrange</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L5" class="blob-num js-line-number" data-line-number="5"></td>
        <td id="file-tokenizer-py-LC5" class="blob-code blob-code-inner js-file-line">tqdm.pandas(<span class="pl-v">desc</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>Progress<span class="pl-pds">'</span></span>)</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L6" class="blob-num js-line-number" data-line-number="6"></td>
        <td id="file-tokenizer-py-LC6" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L7" class="blob-num js-line-number" data-line-number="7"></td>
        <td id="file-tokenizer-py-LC7" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> load spacy tokenizer</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L8" class="blob-num js-line-number" data-line-number="8"></td>
        <td id="file-tokenizer-py-LC8" class="blob-code blob-code-inner js-file-line">nlp <span class="pl-k">=</span> spacy.load(<span class="pl-s"><span class="pl-pds">'</span>en<span class="pl-pds">'</span></span>,<span class="pl-v">disable</span><span class="pl-k">=</span>[<span class="pl-s"><span class="pl-pds">'</span>parser<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>tagger<span class="pl-pds">'</span></span>, <span class="pl-s"><span class="pl-pds">'</span>ner<span class="pl-pds">'</span></span>])</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L9" class="blob-num js-line-number" data-line-number="9"></td>
        <td id="file-tokenizer-py-LC9" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> df.progress_apply is tqdm method for pandas. It shows progress bar for apply function</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L10" class="blob-num js-line-number" data-line-number="10"></td>
        <td id="file-tokenizer-py-LC10" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> remove the leading and trailing spaces</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L11" class="blob-num js-line-number" data-line-number="11"></td>
        <td id="file-tokenizer-py-LC11" class="blob-code blob-code-inner js-file-line">df[<span class="pl-s"><span class="pl-pds">'</span>SentimentText<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> df.SentimentText.progress_apply(<span class="pl-k">lambda</span> <span class="pl-smi">x</span>: x.strip())</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L12" class="blob-num js-line-number" data-line-number="12"></td>
        <td id="file-tokenizer-py-LC12" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L13" class="blob-num js-line-number" data-line-number="13"></td>
        <td id="file-tokenizer-py-LC13" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> build vocabulary and corresponding counts</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L14" class="blob-num js-line-number" data-line-number="14"></td>
        <td id="file-tokenizer-py-LC14" class="blob-code blob-code-inner js-file-line">words <span class="pl-k">=</span> Counter()</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L15" class="blob-num js-line-number" data-line-number="15"></td>
        <td id="file-tokenizer-py-LC15" class="blob-code blob-code-inner js-file-line"><span class="pl-k">for</span> sent <span class="pl-k">in</span> tqdm(df.SentimentText.values):</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L16" class="blob-num js-line-number" data-line-number="16"></td>
        <td id="file-tokenizer-py-LC16" class="blob-code blob-code-inner js-file-line">    words.update(w.text.lower() <span class="pl-k">for</span> w <span class="pl-k">in</span> nlp(sent))</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L17" class="blob-num js-line-number" data-line-number="17"></td>
        <td id="file-tokenizer-py-LC17" class="blob-code blob-code-inner js-file-line">   </td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L18" class="blob-num js-line-number" data-line-number="18"></td>
        <td id="file-tokenizer-py-LC18" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> sort with most frequently occuring words first</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L19" class="blob-num js-line-number" data-line-number="19"></td>
        <td id="file-tokenizer-py-LC19" class="blob-code blob-code-inner js-file-line">words <span class="pl-k">=</span> <span class="pl-c1">sorted</span>(words, <span class="pl-v">key</span><span class="pl-k">=</span>words.get, <span class="pl-v">reverse</span><span class="pl-k">=</span><span class="pl-c1">True</span>)</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L20" class="blob-num js-line-number" data-line-number="20"></td>
        <td id="file-tokenizer-py-LC20" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> add &lt;pad&gt; and &lt;unk&gt; token to vocab which will be used later</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L21" class="blob-num js-line-number" data-line-number="21"></td>
        <td id="file-tokenizer-py-LC21" class="blob-code blob-code-inner js-file-line">words <span class="pl-k">=</span> [<span class="pl-s"><span class="pl-pds">'</span>_PAD<span class="pl-pds">'</span></span>,<span class="pl-s"><span class="pl-pds">'</span>_UNK<span class="pl-pds">'</span></span>] <span class="pl-k">+</span> words</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L22" class="blob-num js-line-number" data-line-number="22"></td>
        <td id="file-tokenizer-py-LC22" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L23" class="blob-num js-line-number" data-line-number="23"></td>
        <td id="file-tokenizer-py-LC23" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> create word to index dictionary and reverse</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L24" class="blob-num js-line-number" data-line-number="24"></td>
        <td id="file-tokenizer-py-LC24" class="blob-code blob-code-inner js-file-line">word2idx <span class="pl-k">=</span> {o:i <span class="pl-k">for</span> i,o <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(words)}</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L25" class="blob-num js-line-number" data-line-number="25"></td>
        <td id="file-tokenizer-py-LC25" class="blob-code blob-code-inner js-file-line">idx2word <span class="pl-k">=</span> {i:o <span class="pl-k">for</span> i,o <span class="pl-k">in</span> <span class="pl-c1">enumerate</span>(words)}</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L26" class="blob-num js-line-number" data-line-number="26"></td>
        <td id="file-tokenizer-py-LC26" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L27" class="blob-num js-line-number" data-line-number="27"></td>
        <td id="file-tokenizer-py-LC27" class="blob-code blob-code-inner js-file-line"><span class="pl-k">def</span> <span class="pl-en">indexer</span>(<span class="pl-smi">s</span>): </td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L28" class="blob-num js-line-number" data-line-number="28"></td>
        <td id="file-tokenizer-py-LC28" class="blob-code blob-code-inner js-file-line">  <span class="pl-k">return</span> [word2idx[w.text.lower()] <span class="pl-k">for</span> w <span class="pl-k">in</span> nlp(s)]</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L29" class="blob-num js-line-number" data-line-number="29"></td>
        <td id="file-tokenizer-py-LC29" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L30" class="blob-num js-line-number" data-line-number="30"></td>
        <td id="file-tokenizer-py-LC30" class="blob-code blob-code-inner js-file-line"><span class="pl-c"><span class="pl-c">#</span> tokenize the tweets and calculate lengths</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L31" class="blob-num js-line-number" data-line-number="31"></td>
        <td id="file-tokenizer-py-LC31" class="blob-code blob-code-inner js-file-line">df[<span class="pl-s"><span class="pl-pds">'</span>sentimentidx<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> df.SentimentText.progress_apply(indexer)</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L32" class="blob-num js-line-number" data-line-number="32"></td>
        <td id="file-tokenizer-py-LC32" class="blob-code blob-code-inner js-file-line">df[<span class="pl-s"><span class="pl-pds">'</span>lengths<span class="pl-pds">'</span></span>] <span class="pl-k">=</span> df.sentimentidx.progress_apply(<span class="pl-c1">len</span>)</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L33" class="blob-num js-line-number" data-line-number="33"></td>
        <td id="file-tokenizer-py-LC33" class="blob-code blob-code-inner js-file-line">
</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L34" class="blob-num js-line-number" data-line-number="34"></td>
        <td id="file-tokenizer-py-LC34" class="blob-code blob-code-inner js-file-line">fig <span class="pl-k">=</span> plt.figure(<span class="pl-v">figsize</span><span class="pl-k">=</span>(<span class="pl-c1">8</span>,<span class="pl-c1">5</span>))</td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L35" class="blob-num js-line-number" data-line-number="35"></td>
        <td id="file-tokenizer-py-LC35" class="blob-code blob-code-inner js-file-line">ax <span class="pl-k">=</span> sns.distplot(df.lengths.values,<span class="pl-v">kde</span><span class="pl-k">=</span><span class="pl-c1">False</span>)<span class="pl-bu">;</span></td>
      </tr>
      <tr>
        <td id="file-tokenizer-py-L36" class="blob-num js-line-number" data-line-number="36"></td>
        <td id="file-tokenizer-py-LC36" class="blob-code blob-code-inner js-file-line">ax.set(<span class="pl-v">xlabel</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>Tweet Length<span class="pl-pds">'</span></span>, <span class="pl-v">ylabel</span><span class="pl-k">=</span><span class="pl-s"><span class="pl-pds">'</span>Frequency<span class="pl-pds">'</span></span>)</td>
      </tr>
</tbody></table>


  </div>

  </div>
</div>

      </div>
      <div class="gist-meta">
        <a href="https://gist.github.com/hpanwar08/c943e38e4d233d07cf38d1886b487ac2/raw/45634871d164b2e46ed11f92b7186137efd17bf9/tokenizer.py" style="float:right">view raw</a>
        <a href="https://gist.github.com/hpanwar08/c943e38e4d233d07cf38d1886b487ac2#file-tokenizer-py">tokenizer.py</a>
        hosted with ❤ by <a href="https://github.com/">GitHub</a>
      </div>
    </div>
</div>
<script>var height = -1; var delayMs = 200;function notifyResize(height) {height = height ? height : document.documentElement.offsetHeight; var resized = false; if (window.donkey && donkey.resize) {donkey.resize(height); resized = true;}if (parent && parent._resizeIframe) {var obj = {iframe: window.frameElement, height: height}; parent._resizeIframe(obj); resized = true;}if (window.location && window.location.hash === "#amp=1" && window.parent && window.parent.postMessage) {window.parent.postMessage({sentinel: "amp", type: "embed-size", height: height}, "*");}if (window.webkit && window.webkit.messageHandlers && window.webkit.messageHandlers.resize) {window.webkit.messageHandlers.resize.postMessage(height); resized = true;}return resized;}function maybeResize() {if (document.documentElement.offsetHeight != height && notifyResize()) {height = document.documentElement.offsetHeight;}delayMs = Math.min(delayMs * 2, 1000000); setTimeout(maybeResize, delayMs);}maybeResize();</script></body></html>