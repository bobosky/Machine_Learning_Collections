This document fills in the questions requested by
https://www.kaggle.com/wiki/WinningModelDocumentationTemplate

See also the code README containing what I thought I should document before
reading Kaggle's guidelines.


Name: Iain Murray
Location: Edinburgh, UK
Email: i.murray@ed.ac.uk


Competition:

1. Summary

The main guiding principle for my approach was that Bayesian statistics would
give the best answer I could come up with, if I could mathematically express my
beliefs about the data, and solve the necessary numerical computations.

The model was built after a very quick look at the data ./examine_singletons. A
standard MCMC baseline method was used to draw different plausible explanations
of the data. The predictions for each sky were then (hackily) optimized, by
making the competition metric as low as possible when evaluated against all the
plausible explanations. (This final fitting procedure isn't quite the optimal
thing to do, because the cost function doesn't separate across the skies.) No
external data or methods were used.


2. Features Selection / Extraction

The only features that were extracted were:

The form of the tangential "force" model:
    f = 1 / (max(radius, r0) / scale),    where mass \propto 1/scale.
that shifts the mean eccentricities (in the way described in the maximum
likelihood benchmark code). The sequence of plots that led to this rule are
documented in the top of ./examine_singletons/play*.m

The unshifted distribution of the eccentricities: zero-mean Gaussian, with
standard deviation 0.22 -- measured directly from the data.

Reasonable ranges for the parameters halo positions, r0, scale, 1/mass,
additional noise within a radius of r0 from the halo. These are coded in
flex_ll.m, and were largely made up, based on my impressions after running the
code in ./examine_singletons, and tweaked after initial runs of the sampler.
Actually these prior ranges (or distributions) could be important, and should
have been learned better.

I'm assuming that none of this is of interest, because the work behind lenstool
will have built much more informed models.


3. Modeling Techniques and Training

The model is a log prior and likelihood combined, up to a constant, in
flex_ll.m. The likelihood is as used in the maximum likelihood example code,
except with the force rule identified above, and additional noise in the halo
centre. The priors were flat (in the parameterization
x,y,1/mass,r0,addition_to_e_sigma) over broad ranges.

Posterior samples were drawn with univariate slice sampling updates (Neal,
2003). The initial bracket width was set to cover the whole prior range, and
only shrinking in adaptation was performed (see paper or code for details).
Depending on the run, there were 4000, 40000, or 100000 sweeps through the
variables.

Prediction fits were optimized by initializing at the median of a window of 100
samples. One should use established, tested, off-the-shelf optimizers. Here's
what I actually did: Each halo was updated in turn. A random direction was
chosen (biased due to a bug in the code marked FIXME) and a point along a line
proposed. The proposal was initially evaluated on a small number of posterior
samples (10, 50, or 100, depending on the run), and a closer point selected if
not acceptable. Once a provisionally acceptable move was found, it was checked
on more posterior samples (1000, 4000, or 16000). This procedure was repeated
for a fixed number of iterations (200, 1000, or 2000).

The first submission used the smallest numbers of runs for each choice listed
above, and limited the additional core noise to be very small (0.01). The last
submission allowed the e1 and e2 stand deviation to vary substantially (up to
0.22+0.3), and used multiple longer runs (with numbers varying as above). The
final prediction was picked by evaluating the Dark World's metric on all of the
posterior samples generated by all runs that used the noisy core model.


4. Code Description

For more information on the code, see README and comments within it.

The first submission was generated with the following Matlab/Octave scripts and
functions:

    do_all.m -- calls sampler, then fits predictions, and writes them out

    test_sample.m -- generates samples
    flex_ll.m -- defines the posterior distribution
    slice_sample.m -- generic MCMC routine
    
    test_fit_pred.m -- fits the point prediction. Uses:
    dw_metric.m -- Matlab port of cost function for fitting purposes
    hack_opt.m -- random line-searches routine (don't use!)

    test_write_pred.m -- writes out test_stab.csv predictions

Inputs and outputs to functions are documented at the top of each of them.
The scripts can just be run (after data is pre-processed with prepare_data.sh).


5. How To Generate the Solution

The core instructions, extracted from my README file:

1. Copy the challenge .csv files into ./data
2. Run ./prepare_data.sh
   This script should work on a Unix-like system with bash and python.
   It trivially rewrites the data for loading into Matlab/Octave with minimum fuss.
3. In Matlab or Octave (slower, but free software), run do_all

*Eventually* you'll get test_stab.csv -- it won't be quite the same as my first
submission because of differences in random seeds.

For later submissions I tweaked parts of the code to run for longer, and did
multiple runs, moving out tsamples and tpred in between. These results
directories were combined with other_code/test_merge_final.m.

More details in README.


6. Additional Comments and Observations

I didn't try any other models / methods. My impression was that optimizing the
cost function was the only thing, if anything, I could bring to this
competition. It seemed that cosmologists were already doing sophisticated MCMC
with sophisticated models. Indeed optimizing the cost did seem to make a big
difference to score: before I wrote the optimization code, I had a stub that
took random samples or posterior means, which performed much worse on fantasy
data.

Optimizing the cost could have been done better, but I would first want
convincing that the cost function was the right task to be optimizing before
working harder on that. Interactively plotting time series of the samples, some
of the halo posteriors were much more variable than others: point estimates
throws away this knowledge. I would try to optimize a more complete description
of the posterior for use in later analysis.


7. Figures

I didn't generate any figures worthy of note.


8. References

I provided a write-up at http://homepages.inf.ed.ac.uk/imurray2/pub/12kaggle_dark/
As stated there, I have a tutorial exercise that explains how slice sampling
could be used in another data analysis problem at:
    http://homepages.inf.ed.ac.uk/imurray2/teaching/09mlss/
The original slice sampling paper is at:
    http://www.cs.toronto.edu/~radford/slice-aos.abstract.html
and a shorted introduction is in Chapter 29 of David MacKay's book:
    http://www.inference.phy.cam.ac.uk/mackay/itila/book.html

